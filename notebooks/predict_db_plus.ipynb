{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_database():\n",
    "    print('실행시작!')\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from sqlalchemy import create_engine\n",
    "    import psycopg2\n",
    "    import requests\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    #기준데이터프레임호출 => 가지고 있는 대여소관련 데이터정보.\n",
    "    base_df = pd.read_csv('./all_all.csv')\n",
    "\n",
    "    base_df.drop(['일시','거치율','거치대수','대여소id','주소','잔여대수'],axis=1,inplace=True)\n",
    "    base_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    #api key를 사용해서 경도와 위도를 넣고 주소를 반환하는 함수\n",
    "    def get_addr(X, Y):\n",
    "        '''\n",
    "        X,Y에 경도,위도를 넣으면 주소를 return 한다.\n",
    "        '''\n",
    "        apiKey='6D89790B-9BFE-36C5-8776-9C7DD60ACC62'\n",
    "        r =requests.get(f'http://apis.vworld.kr/coord2jibun.do?x={X}&y={Y}\\\n",
    "        &apiKey={apiKey}&domain=http://map.vworld.kr/&output=json')\n",
    "        location = r.json()\n",
    "    #     print(location)\n",
    "        return location['ADDR']\n",
    "\n",
    "    ### 자치구, QR여부 추가\n",
    "    def ready_for_preprocessing(Benchmark):\n",
    "        '''\n",
    "        Benchmark에 비교군 데이터프레임을 넣으면\n",
    "        현재 API에서 불러온 거치소 데이터와 비교해서\n",
    "        새로 신설된 거치소에 대한 '자치구', '운영방식'을 추가 해준다.\n",
    "        비교군(Benchmark)에서 필요한 column: ['대여소번호','자치구','운영방식']\n",
    "        '''\n",
    "        # 시간 설정\n",
    "        now = datetime.now()\n",
    "        hour = now.hour\n",
    "        min_ = (now.minute // 10) * 10\n",
    "        now_date = now.strftime('%Y-%m-%d')\n",
    "        ago24 = now - timedelta(hours = 24)\n",
    "        ago48 = now - timedelta(hours = 48)\n",
    "        check_period = now_date + f' {hour}:{min_}'  \n",
    "        # DB postgres 엔진 객체 설정\n",
    "        engine = create_engine(\"postgresql://postgres:6team123!@restored-aurora.cj92narf3bwn.ap-northeast-2.rds.amazonaws.com:5432/final_project\")\n",
    "        # API 호출\n",
    "        Bike_api = pd.read_sql_query(f\"\"\"SELECT *\n",
    "        FROM bike WHERE 일시 >= '{check_period}';\n",
    "        \"\"\", con=engine.connect())\n",
    "        # API 자전거 거치대 unique 값\n",
    "        Bike_api = Bike_api[['대여소이름','위도','경도']].drop_duplicates('대여소이름')\n",
    "        Bike_api['대여소번호'] = Bike_api['대여소이름'].apply(lambda x: x.split('.')[0]).astype('int32')\n",
    "        # 필요 set 선언\n",
    "        Benchmark_set = set(Benchmark['대여소번호'])\n",
    "        Bike_api_set = set(Bike_api['대여소번호'])\n",
    "        Deleted_node_set = Benchmark_set - Bike_api_set\n",
    "        New_node_set = Bike_api_set - Benchmark_set\n",
    "        New_node = Bike_api[Bike_api['대여소번호'].isin(New_node_set)]\n",
    "        Old_node = Bike_api[~Bike_api['대여소번호'].isin(New_node_set)]\n",
    "        # 안내문\n",
    "        print(f'[안내] 삭제된 거치소가 {len(Deleted_node_set)}개 입니다.')\n",
    "        print(f'[안내] 기준 거치소 갯수: {len(Benchmark_set)} , 실시간 거치소 갯수:{len(Bike_api_set)}')\n",
    "        print(f'[안내] 삭제된 거치소 리스트:')\n",
    "        print(Deleted_node_set)\n",
    "        if len(New_node_set) == 0:\n",
    "            print('[안내] 신규 추가 거치소가 없습니다.')\n",
    "        else:\n",
    "            print(f'\\n[안내] 신규 거치소가 {len(New_node_set)}개 입니다.')\n",
    "            print(f'[안내] 신규 거치소 리스트:')\n",
    "        # 신규 거치소의 운영방식은 모두 QR임.\n",
    "            New_node['운영방식'] = 'QR'\n",
    "            for i, d, r, z in zip(New_node.index, New_node['대여소이름'], New_node['경도'], New_node['위도']):\n",
    "                print(f'{i}, {d}')\n",
    "                print(f'{r, z}, {get_addr(r,z)}')\n",
    "                try:\n",
    "                    New_node.loc[i,'자치구'] = get_addr(New_node.loc[i,'경도'], New_node.loc[i,'위도']).split(' ')[1]\n",
    "                except:\n",
    "                    print(f'위경도가 잘못된 거치소 입니다: {d}')\n",
    "                    pass\n",
    "        Bike_api.reset_index(drop=True,inplace=True)\n",
    "        New_node.reset_index(drop=True,inplace=True)\n",
    "    #     Node_Whole = pd.concat([New_node, Old_node], axis = 0)\n",
    "        return Bike_api, New_node, Deleted_node_set\n",
    "    #Bike_api => 현재시점 모든 거치소dataFrame\n",
    "    #New_node => 기준거치소와 현재시점 비교 후 나온 신규 거치소dataFarme\n",
    "    #Deleted_node_set => 철거된 거치소 set형식\n",
    "\n",
    "    #함수 사용할때 세가지 다 선언 해줘야한다.\n",
    "\n",
    "    #Bike_api => 현재시점 모든 거치소dataFrame\n",
    "    #New_node => 기준거치소와 현재시점 비교 후 나온 신규 거치소dataFarme\n",
    "    #Deleted_node_set => 철거된 거치소 set형식\n",
    "    #함수 입력은 기준df\n",
    "    #함수 사용할때 세가지 다 선언 해줘야한다.\n",
    "\n",
    "    #실시간 거치소데이터, 추가된 거치소데이터, 삭제된 거치소데이터\n",
    "    now_df, new_df, Deleted_node_set = ready_for_preprocessing(base_df)\n",
    "\n",
    "    from haversine import haversine\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    #거치소간 최단거리 구하는 문제\n",
    "    def bike_shortest_length(new_df,all_df,lat,lng): \n",
    "        #k => 데이터프레임내 위도 컬럼인덱스\n",
    "        #j => 데이터프레임내 경도 컬럼인덱스\n",
    "        #df = 따릉이거치소 위도경도가 담겨있는 데이터프레임\n",
    "        #(로우마다 각 하나씩의 거치소가 있는 데이터프레임)\n",
    "\n",
    "        #빈데이터프레임 선언\n",
    "        total_total_bike_length = pd.DataFrame()\n",
    "        for i in range(len(new_df)):\n",
    "            length_list = []\n",
    "            for j in range(len(new_df)):\n",
    "                #인덱스가 같으면 두 지점이 같은 위치가 되어서 최단거리가 0이 되기에 i!=j \n",
    "                if new_df.iloc[i,0]!=all_df.iloc[j,0]: \n",
    "\n",
    "                        X = (new_df.iloc[i,lat],new_df.iloc[i,lng]) #(위도,경도)\n",
    "                        Y = (all_df.iloc[j,lat],all_df.iloc[j,lng])\n",
    "                        a = haversine(X, Y, unit = 'm')\n",
    "                        length_list.append(a)\n",
    "                else:\n",
    "                    pass\n",
    "            #최단거리를 정렬해서 그중 인덱스0 -> 최단거리를 뽑아냄\n",
    "            shortest_length = sorted(length_list)[0]\n",
    "            total = pd.DataFrame({\"X\":[i],\"shortest_length\":[shortest_length]})\n",
    "            total_total_bike_length = total_total_bike_length.append(total)\n",
    "            bike_len = total_total_bike_length[['X','shortest_length']]\n",
    "            bike_len.columns = ['대여소번호','bike_shortest']\n",
    "        return bike_len\n",
    "\n",
    "    def bike_to_shortest_length_all(new_df,b_lat,b_lng,another_df,a_lat,a_lng):\n",
    "        #bike_df => 따릉이 데이터프레임\n",
    "        #b_lat => 따릉이 데이터프레임 내 위도 컬럼인덱스\n",
    "        #b_lng => 따릉이 데이터프레임 내 경도 컬럼인덱스\n",
    "        #another_df =>시설물 데이터프레임\n",
    "        #a_lat => 시설물 위도\n",
    "        #a_lng => 시설물 위도\n",
    "\n",
    "        #빈데이터프레임 선언\n",
    "        total_total_length = pd.DataFrame()\n",
    "        #df = 각거치소들이 각 로우에 들어가있는 데이터프레임(위도경도 있어야함)\n",
    "        for i in range(len(new_df)):\n",
    "            length_list = []\n",
    "            for j in range(len(another_df)):\n",
    "                X = (new_df.iloc[i,b_lat],new_df.iloc[i,b_lng])\n",
    "                Y = (another_df.iloc[j,a_lat],another_df.iloc[j,a_lng])\n",
    "                a = haversine(X, Y, unit = 'm')\n",
    "                length_list.append(a)\n",
    "            shortest_length = sorted(length_list)[0]\n",
    "            total = pd.DataFrame({\"X\":[i],\"shortest_length\":[shortest_length]})\n",
    "            total_total_length = total_total_length.append(total)\n",
    "        return total_total_length\n",
    "\n",
    "    def df_concat(df,all_df,df_lat,df_lng):\n",
    "\n",
    "        bike_len = bike_shortest_length(df,all_df,df_lat,df_lng)\n",
    "\n",
    "        #시장과의거리\n",
    "        market = pd.read_csv('./dareng_data/전통시장전처리후.csv')\n",
    "        market_len = bike_to_shortest_length_all(df,df_lat,df_lng,market,-1,-2)\n",
    "        market_len.columns = ['X', 'market_shortest']\n",
    "\n",
    "        #공원과의거리\n",
    "        park = pd.read_csv('./dareng_data/공원전처리후.csv')\n",
    "        park_len = bike_to_shortest_length_all(df,df_lat,df_lng,park,4,5)\n",
    "        park_len.columns = ['X', 'park_shortest']\n",
    "\n",
    "        #지하철과의거리\n",
    "        subway = pd.read_csv('./dareng_data/지하철전처리후.csv')\n",
    "        subway_len = bike_to_shortest_length_all(df,df_lat,df_lng,subway,3,4)\n",
    "        subway_len.columns = ['X', 'subway_shortest']\n",
    "\n",
    "        #학교와의거리\n",
    "        school = pd.read_csv('./dareng_data/중고등대(원).csv')\n",
    "        school_len = bike_to_shortest_length_all(df,df_lat,df_lng,school,-2,-1)\n",
    "        school_len.columns = ['X', 'school_shortest']\n",
    "\n",
    "        #문화공간과의거리\n",
    "        culture = pd.read_csv('./dareng_data/문화공간전처리후.csv')\n",
    "        culture_len = bike_to_shortest_length_all(df,df_lat,df_lng,culture,-2,-1)\n",
    "        culture_len.columns = ['X', 'culture_shortest']\n",
    "\n",
    "        #버스와의거리\n",
    "        bus = pd.read_csv('./dareng_data/bus.csv')\n",
    "        bus_len = bike_to_shortest_length_all(df,df_lat,df_lng,bus,-1,-2)\n",
    "        bus_len.columns = ['X', 'bus_shortest']\n",
    "\n",
    "        #합치기\n",
    "        bike_len['park_shortest'] = park_len['park_shortest']\n",
    "        bike_len['subway_shortest'] = subway_len[\"subway_shortest\"]\n",
    "        bike_len['school_shortest'] = school_len['school_shortest']\n",
    "        bike_len['culture_shortest'] = culture_len['culture_shortest']\n",
    "        bike_len['bus_shortest'] = bus_len['bus_shortest']\n",
    "        bike_len['market_shortest'] = market_len['market_shortest']\n",
    "\n",
    "        return bike_len\n",
    "\n",
    "    #자전거거치소로부터 또다른 거치소 및 다른 시설물과의 최단거리를 구하여 파생변수 dataframe 생성해주는 함수\n",
    "    bike_length = df_concat(new_df,now_df,1,2)\n",
    "    bike_length.reset_index(drop=True,inplace=True)\n",
    "    bike_length.drop('대여소번호',axis=1,inplace=True)\n",
    "\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point, Polygon, LineString, MultiLineString   \n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore') \n",
    "    def in400m(bike_geo, another_geo):\n",
    "\n",
    "        total_total = pd.DataFrame()\n",
    "        bike_loc_list = bike_geo.대여소번호.unique().tolist()\n",
    "        #bike_geo = gpd.GeoDataFrame(bike_geo, crs='epsg:4326', geometry='geometry')\n",
    "        #another_geo = gpd.GeoDataFrame(another_geo, crs='epsg:4326', geometry='geometry')\n",
    "\n",
    "        for bike_num in bike_loc_list:\n",
    "            data = bike_geo[bike_geo.대여소번호 == bike_num]\n",
    "            #뽑은 대여소를 점에서 400m범위를 주고 면적으로 변경한다.\n",
    "            data_b = data.buffer(1.0247175113158795e-05*400)\n",
    "            df = data_b\n",
    "\n",
    "            bk_to_another_list = []\n",
    "            for i in range(len(another_geo)):\n",
    "                #거치소 하나의 면적에서 다른시설 위치정보 전체를 돌면서 포함여부를 판단한다.\n",
    "                if df.geometry.contains(another_geo.loc[i,'geometry']).sum()!=0:\n",
    "                    bk_to_another_list.append(1)\n",
    "                elif df.geometry.contains(another_geo.loc[i,'geometry']).sum()==0:\n",
    "                    bk_to_another_list.append(0)\n",
    "\n",
    "            total = pd.DataFrame({'거치소범위포함여부':bk_to_another_list})\n",
    "            total['거치소'] = bike_num\n",
    "            total_total = total_total.append(total)\n",
    "\n",
    "        count_df = total_total.groupby('거치소',as_index=False).거치소범위포함여부.sum()\n",
    "        return count_df[['거치소','거치소범위포함여부']]\n",
    "\n",
    "    def in400_garosu_v(bike_geo):\n",
    "        garosu = gpd.read_file('./new_data/Garosugil.csv')\n",
    "        garosu.가로수길길이 = garosu.가로수길길이.astype(float)\n",
    "        garosu = garosu[garosu.가로수길길이 > 3.6]\n",
    "        garosu['가로수길종료위도'] = garosu.가로수길종료위도.astype(float)\n",
    "        garosu['가로수길종료경도'] = garosu.가로수길종료경도.astype(float)\n",
    "\n",
    "        garosu['가로수길시작위도'] = garosu.가로수길시작위도.astype(float)\n",
    "        garosu['가로수길시작경도'] = garosu.가로수길시작경도.astype(float)\n",
    "\n",
    "        garosu = garosu[(garosu.가로수길종료위도 < 37.413294) == False]\n",
    "\n",
    "        garosu = garosu[(garosu.가로수길종료위도 > 37.715133)==False]\n",
    "        garosu.drop_duplicates(keep='first', inplace=True)\n",
    "        garosu['geometry_1'] = garosu.apply(lambda row: Point(row['가로수길시작경도'], row['가로수길시작위도']), axis=1)\n",
    "\n",
    "        garosu['geometry_2'] = garosu.apply(lambda row: Point(row['가로수길종료경도'], row['가로수길종료위도']), axis=1)\n",
    "\n",
    "        garosu['geometry'] = garosu.apply(lambda row: LineString([row['geometry_1'], row['geometry_2']]), axis=1)\n",
    "\n",
    "        for garosugil in garosu.가로수길명.unique().tolist():\n",
    "            if len(garosu[garosu.가로수길명 == garosugil]) >1:\n",
    "                #print('yes')\n",
    "                df = garosu[garosu.가로수길명 == garosugil].reset_index(drop=True)\n",
    "                #멀티라인스트링값  \n",
    "                garosu.loc[garosu.가로수길명 == garosugil,'geometry'] = garosu.apply(lambda x: MultiLineString(df.geometry.values) if x['가로수길명']==garosugil else x['가로수길명'], axis=1)\n",
    "\n",
    "        garosu.drop_duplicates(['geometry'],keep='first',inplace=True)\n",
    "        garosu.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        total_total_garosu = pd.DataFrame()\n",
    "        bike_loc_list = bike_geo.대여소번호.unique().tolist()\n",
    "        garosu_unique_list= garosu.가로수길명.tolist()\n",
    "        for bike_num in bike_loc_list:\n",
    "            #자전거 대여소 하나만 뽑는다.\n",
    "            locals()['bike_'+f'{bike_num}'] = bike_geo[bike_geo.대여소번호 == bike_num]\n",
    "            #뽑은 대여소를 점에서 400m범위를 주고 면적으로 변경한다.\n",
    "            locals()['bike_'+f'{bike_num}'+'_b'] = locals()['bike_'+f'{bike_num}'].buffer(1.0247175113158795e-05*400)\n",
    "            df = locals()['bike_'+f'{bike_num}'+'_b']\n",
    "\n",
    "            bk_to_garosu_list = []\n",
    "            for i in range(len(garosu)):\n",
    "                #거치소 하나의 면적에서 마켓들 전체를 돌면서 포함여부를 판단한다.\n",
    "                if df.geometry.intersects(garosu.loc[i,'geometry']).sum()!=0:\n",
    "                    bk_to_garosu_list.append(1)\n",
    "                elif df.geometry.intersects(garosu.loc[i,'geometry']).sum()==0:\n",
    "                    bk_to_garosu_list.append(0)\n",
    "\n",
    "            total = pd.DataFrame({'거치소범위포함여부':bk_to_garosu_list})\n",
    "            total['거치소'] = bike_num\n",
    "            total_total_garosu = total_total_garosu.append(total)\n",
    "        count_df = total_total_garosu.groupby('거치소',as_index=False).거치소범위포함여부.sum()\n",
    "        return count_df     \n",
    "\n",
    "    def df_concat_in400(df,all_df):\n",
    "\n",
    "        df['geometry'] = df.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        df = gpd.GeoDataFrame(df, crs='epsg:4326', geometry='geometry')\n",
    "\n",
    "        all_df['geometry'] = all_df.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        all_df = gpd.GeoDataFrame(all_df, crs='epsg:4326', geometry='geometry')\n",
    "\n",
    "        market = pd.read_csv('./dareng_data/전통시장전처리후.csv')\n",
    "        market['geometry'] = market.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        market_geo = gpd.GeoDataFrame(market, crs='epsg:4326', geometry='geometry')\n",
    "        #market_geo.to_csv('./market_geo.csv',index=False)\n",
    "\n",
    "        park = pd.read_csv('./dareng_data/공원전처리후.csv')\n",
    "        park['geometry'] = park.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        park_geo = gpd.GeoDataFrame(park, crs='epsg:4326', geometry='geometry')\n",
    "        #park_geo.to_csv('./park_geo.csv',index=False)\n",
    "\n",
    "        subway = pd.read_csv('./dareng_data/지하철전처리후.csv')\n",
    "        subway['geometry'] = subway.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        subway_geo = gpd.GeoDataFrame(subway, crs='epsg:4326', geometry='geometry')\n",
    "        #subway_geo.to_csv('./subway_geo.csv',index=False)\n",
    "\n",
    "        school = pd.read_csv('./dareng_data/중고등대(원).csv')\n",
    "        school['geometry'] = school.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        school_geo = gpd.GeoDataFrame(school, crs='epsg:4326', geometry='geometry')\n",
    "        #school_geo.to_csv('./school_geo.csv',index=False)\n",
    "\n",
    "        culture = pd.read_csv('./dareng_data/문화공간전처리후.csv')\n",
    "        culture['geometry'] = culture.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        culture_geo = gpd.GeoDataFrame(culture, crs='epsg:4326', geometry='geometry')\n",
    "        #culture_geo.to_csv('./culture_place_geo.csv',index=False)\n",
    "\n",
    "        bus = pd.read_csv('./dareng_data/bus.csv')\n",
    "        bus['geometry'] = bus.apply(lambda row: Point(row['경도'], row['위도']), axis=1)\n",
    "        bus_geo = gpd.GeoDataFrame(bus, crs='epsg:4326', geometry='geometry')\n",
    "        #bus_geo.to_csv('./bus_geo.csv',index=False)\n",
    "\n",
    "        ############################\n",
    "        with open('./hangang.json', \"r\") as json_file:\n",
    "            tmp = json.load(json_file)\n",
    "        tmp = gpd.read_file(tmp)\n",
    "\n",
    "        #한강의 도로 선을 buffer을 통해 400m 범위를 준다.\n",
    "        tmp['geometry'] = tmp.buffer(1.0247175113158795e-05*400)\n",
    "        #400m 늘어난 한강도로 범위 안에 따릉이가 포함되어있는지 여부를 판별\n",
    "        hangan_bike_loc_list = []\n",
    "        for i in range(len(df)):\n",
    "            if tmp.geometry.contains(df.loc[i,'geometry']).sum()!=0:\n",
    "                hangan_bike_loc_list.append(1)\n",
    "            elif tmp.geometry.contains(df.loc[i,'geometry']).sum()==0:  \n",
    "                hangan_bike_loc_list.append(0)\n",
    "\n",
    "        #############################\n",
    "\n",
    "        bike_count = in400m(df, all_df)\n",
    "        bike_count.columns = ['대여소번호','in400_bike']\n",
    "\n",
    "        market_count = in400m(df, market_geo)\n",
    "        market_count.columns = ['대여소번호','in400_market']\n",
    "\n",
    "        park_count = in400m(df, park_geo)\n",
    "        park_count.columns = ['대여소번호','in400_park']\n",
    "\n",
    "        subway_count = in400m(df, subway_geo)\n",
    "        subway_count.columns = ['대여소번호','in400_subway']\n",
    "\n",
    "        school_count = in400m(df, school_geo)\n",
    "        school_count.columns = ['대여소번호','in400_school']\n",
    "\n",
    "        culture_count = in400m(df, culture_geo)\n",
    "        culture_count.columns = ['대여소번호','in400_culture']\n",
    "\n",
    "        bus_count = in400m(df, bus_geo)\n",
    "        bus_count.columns = ['대여소번호','in400_bus']\n",
    "\n",
    "        garosu_count = in400_garosu_v(df)\n",
    "        garosu_count.columns = ['대여소번호','in400_garosu']                                   \n",
    "\n",
    "        #합치기\n",
    "        bike_count['in400_market'] = market_count['in400_market']\n",
    "        bike_count['in400_park'] = park_count[\"in400_park\"]\n",
    "        bike_count['in400_subway'] = subway_count['in400_subway']\n",
    "        bike_count['in400_school'] = school_count['in400_school']\n",
    "        bike_count['in400_culture'] = culture_count['in400_culture']\n",
    "        bike_count['in400_bus'] = bus_count['in400_bus']\n",
    "        bike_count['in400_garosu'] = garosu_count['in400_garosu']\n",
    "        bike_count['in400_hangang'] = hangan_bike_loc_list\n",
    "        return bike_count\n",
    "\n",
    "\n",
    "\n",
    "    #자전거 거치소로부터 400m내 거치소 및 다른 시설물들의 포함 개수 파생변수를 생성해주는 함수\n",
    "    bike_count = df_concat_in400(new_df,now_df)\n",
    "\n",
    "\n",
    "    #(최단거리 파생변수 + 400m내 시설물개수 파생변수 )합치기\n",
    "    bike = pd.concat([bike_count,bike_length],axis=1)\n",
    "\n",
    "    #기존에 있던 거치소데이터에는 파생변수가 있으나 새로운 거치소는 파생변수가 없으니 위의 함수에서 구한 내용을 merge를 통해 붙여준다.\n",
    "    new = pd.merge(new_df,bike, on='대여소번호',how=\"left\")\n",
    "\n",
    "    #기존에 만들어져있는 자치구관련 변수들을 데이터프레임으로 만들어줬고 거치소들의 자치구에 맞추어 merge하고 파생변수를 붙여준다.\n",
    "    jachigu = pd.read_csv('./dareng_data/jachigu.csv')\n",
    "    new_jachigu_plus = pd.merge(new,jachigu,on='자치구',how='left')\n",
    "\n",
    "    #nowfinal에 concat하기 위햔 컬럼 정렬\n",
    "    new_jachigu_plus = new_jachigu_plus[base_df.columns.tolist()]\n",
    "\n",
    "    #기존에 데이터레임에 파생변수가 추가된 새로운 거치소 정보 업데이트\n",
    "    now_final = pd.concat([base_df,new_jachigu_plus],axis=0)\n",
    "\n",
    "    #now_final - Deleted_node_set\n",
    "    #삭제된 거치소들을 가지고 있는 거치소관련 데이터프레임에서 삭제\n",
    "    if len(Deleted_node_set) != 0:\n",
    "        now_final = now_final[now_final.대여소번호.isin(list(Deleted_node_set)) == False]\n",
    "\n",
    "    #now_final 관련 결측치 및 전처리.\n",
    "    now_final.세대수 = now_final.세대수.apply(lambda x : int(x.replace(',','')))\n",
    "    now_final.거주자총인구수 = now_final.거주자총인구수.apply(lambda x : int(x.replace(',','')))\n",
    "    now_final['20s'] = now_final['20s'].apply(lambda x : int(x.replace(',','')))\n",
    "    now_final['30s'] = now_final['30s'].apply(lambda x : int(x.replace(',','')))\n",
    "\n",
    "    import psycopg2\n",
    "    import sqlalchemy\n",
    "    import pandas as pd\n",
    "\n",
    "    # DB postgres 엔진 객체 설정\n",
    "    engine = create_engine(\"postgresql://postgres:6team123!@restored-aurora.cj92narf3bwn.ap-northeast-2.rds.amazonaws.com:5432/final_project\")\n",
    "\n",
    "    df = pd.read_sql_query(f\"\"\"SELECT * FROM bike WHERE 일시 BETWEEN (now() - interval '2 day'  - interval '2 hours' + interval '9 hour') AND (now() + interval '9 hour') AND extract(minute from 일시) in (50, 00, 10, 20, 30, 40);\n",
    "        \"\"\", con=engine.connect())\n",
    "\n",
    "    #다른 실시간 데이터와 구분짓기 위해서 \"시간명+자전거\"로 컬럼명을 설정해줌,\n",
    "    df['일시'] = df['일시'].apply(lambda x : x.strftime('%Y-%m-%d %H:%M') + '자전거')\n",
    "\n",
    "    #대여소번호만 따로 구하기 위해서 대여소명에서 앞 숫자만떼어내서 대여소번호 컬럼으로 설정\n",
    "    df['대여소번호'] = df.대여소이름.apply(lambda x : x.split('.')[0])\n",
    "\n",
    "    #pivot_table을 통해서 row에 있던 시간들을 컬럼으로 올려준다.\n",
    "    bike_pivot = pd.pivot_table(df, index = '대여소번호',columns = '일시', values = '잔여대수').reset_index()\n",
    "\n",
    "    #실시간 데이터를 불러온 기간내에 중간에 새로생긴 거치소의 경우에는 결측치가 생긴다.\n",
    "    #결측치는 곧 존재하지 않았기에 값이 존재하지 않는 것이기에 0으로 채워준다.\n",
    "    bike_pivot = bike_pivot.fillna(0)\n",
    "\n",
    "    #대여소번호컬럼 -> 인덱스기준으로 맨 앞에 위치\n",
    "    bike_series = bike_pivot.iloc[:,0]\n",
    "\n",
    "    #대여소번호 컬럼만 있는 데이터프레임\n",
    "    bike_series = pd.DataFrame(bike_series) \n",
    "\n",
    "    # 가장 최근 데이터 기준 288개 컬럼 맞춘 dataframe\n",
    "    bike_pivot_setting = bike_pivot.iloc[:,-288:] \n",
    "\n",
    "    # 대여소번호와 시간데이터 288개를 합한 총 289개의 columns를 가진 데이터=train data\n",
    "    bike_final = bike_series.join(bike_pivot_setting) \n",
    "\n",
    "    df_total = bike_final\n",
    "    #문자열 슬라이싱을 통해 대여소번호를 구했기때문에 str형으로 존재 -> int형으로 바꾸어준다.\n",
    "    df_total.대여소번호 = df_total.대여소번호.astype(int)\n",
    "\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    import psycopg2\n",
    "    import sqlalchemy\n",
    "    import pandas as pd\n",
    "    #현재시간(지금 찍히는 시간)\n",
    "    now = datetime.now() \n",
    "\n",
    "    #날씨관련한 데이터를 가져오는데 시간에 따라 가져오는 수를 다르게 하는 함수.\n",
    "    def read_line(now):\n",
    "        #현재시간기준 - 48시간전  #datetime.timedelta(1) = 하루\n",
    "        twodaysago = now - 2 * timedelta(1)\n",
    "        #twodaysago를 문자열로 변환\n",
    "        twodaysago_result = twodaysago.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "        #twodaysago_result에서 한시간을 더 뺌 => 49시간\n",
    "        twodaysago_same_result = twodaysago - timedelta(hours=1)\n",
    "        #twodaysago_same_result를 문자열로 변환\n",
    "        twodaysago_before_hour = twodaysago_same_result.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "        line = now.strftime('%M')\n",
    "\n",
    "        if line < \"50\" :\n",
    "            return twodaysago_before_hour\n",
    "        else:\n",
    "            return twodaysago_result\n",
    "\n",
    "    #퀴리안에 들어갈 내용    \n",
    "    result = read_line(now)\n",
    "\n",
    "    #쿼리날려 데이터 가져오기\n",
    "    table_weather = pd.read_sql_query(f\"\"\"SELECT 관측일시, 권역명, 체감온도, 날씨\n",
    "        FROM weather WHERE 관측일시 > '{result}';\n",
    "        \"\"\", con=engine.connect())\n",
    "\n",
    "    #관측일시 컬럼 문자열로 수정.\n",
    "    table_weather['관측일시'] = table_weather['관측일시'].apply(lambda x : x.strftime('%Y-%m-%d %H:%M') + '날씨')\n",
    "\n",
    "    #체감온도 데이터만 뽑기\n",
    "    feel_like = table_weather[['관측일시', '권역명', '체감온도']]\n",
    "    #체감온도 데이터 row에서 컬럼으로 변환\n",
    "    how_many_column = pd.pivot_table(feel_like, index = '권역명',columns = '관측일시', values = '체감온도').reset_index()\n",
    "\n",
    "    #날씨 데이터만 뽑기\n",
    "    wwweather = table_weather[['관측일시', '권역명', '날씨']]\n",
    "\n",
    "    #날씨 컬럼 value값 Thunderstorm, Drizzle, Rain, Snow, Atmosphere, Clear, Clouds\n",
    "    #날씨의 경우는 자전거를 타는데 제약이 있는 날씨들은 0을 주고 그외에는(clear, clouds) 다 1을 주었다.\n",
    "    def func(wwweather):\n",
    "        if wwweather['날씨'] == 'Clear':\n",
    "            return 1\n",
    "        elif wwweather['날씨'] == 'Clouds':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    #날씨 컬럼 관련 value값 위의 함수를 적용해 수정\n",
    "    wwweather['날씨'] = wwweather.apply(func, axis=1)\n",
    "\n",
    "    #날씨 컬럼의 시간별 데이터를 row에서 column 으로 변경\n",
    "    how_many_columns = pd.pivot_table(wwweather, index = '권역명',columns = '관측일시', values = '날씨', aggfunc=lambda x: ' '.join(str(v) for v in x)).reset_index()\n",
    "\n",
    "    #모든 데이터들을 다 int로 변경\n",
    "    for col in how_many_columns.columns.tolist()[1:]:\n",
    "        how_many_columns[col] = how_many_columns[col].apply(lambda x: int(x))\n",
    "\n",
    "    from datetime import datetime\n",
    "    import psycopg2\n",
    "    import sqlalchemy\n",
    "    import pandas as pd\n",
    "\n",
    "    #현시점 기준으로 실시간 데이터불러오기\n",
    "    table_dust = pd.read_sql_query(f\"\"\"SELECT 일시, 측정소명, 통합대기환경등급 FROM dust WHERE 일시 BETWEEN (now() - interval '2 day' -interval '7 hours'+ interval '9 hour') AND (now() + interval '9 hour') AND extract(minute from 일시) in (00);\n",
    "        \"\"\", con=engine.connect())\n",
    "\n",
    "    #컬럼명 재설정\n",
    "    table_dust.rename(columns = {\"측정소명\" : \"권역명\", \"일시\" : \"관측일시\"}, inplace = True)\n",
    "\n",
    "    #관측일시 이름을 파생변수화.\n",
    "    table_dust['관측일시'] = table_dust['관측일시'].apply(lambda x : x.strftime('%Y-%m-%d %H:%M') + '미세먼지')\n",
    "\n",
    "    # 현재 코로나로 인해 마스크를 착용하지 않고 밖에 나가는 경우는 극히 드물다. 따라서 어느정도 미세먼지에 둔감.\n",
    "    # 서울은 미세먼지가 심한 도시중 한 곳이며, 현재 많은 사람들은 미세먼지에 어느정도 적응해있기 때문에 \n",
    "    # 좋음,보통,나쁨 정도의 미세먼지는 자전거를 이용하는데 영향을 주기 어렵다고 판다. \n",
    "    # 그 이외인 매우나쁨의 경우에는 자전거 이용에 제약이 생길것으로 판단.\n",
    "    def func(table_dust):\n",
    "        if table_dust['통합대기환경등급'] == '좋음':\n",
    "            return 'good'\n",
    "        elif table_dust['통합대기환경등급'] == '보통':\n",
    "            return 'good'\n",
    "        elif table_dust['통합대기환경등급'] == '나쁨':\n",
    "            return 'good'\n",
    "        elif table_dust['통합대기환경등급'] == '점검중':\n",
    "            return '점검중'\n",
    "        else:\n",
    "            return 'bad'\n",
    "\n",
    "    #통합대기환경 등급 위의 함수로 수정\n",
    "    table_dust['통합대기환경등급'] = table_dust.apply(func, axis=1)\n",
    "\n",
    "    #pivot_table을 통해 row에 있던 시간을 컬럼으로 올려 변수화 진행.\n",
    "    dust_pivot_data = pd.pivot_table(table_dust, index = '권역명',columns = '관측일시', values = '통합대기환경등급', aggfunc=lambda x: ' '.join(str(v) for v in x)).reset_index()\n",
    "\n",
    "    #데이터프레임 내에 결측치로는 특정 시간의 각 자치구별 데이터내에 '점검중'이라는 값이 있다.\n",
    "    #그 경우에는 그 시간대 내에 다른 자치구들의 날씨값중에 가장 빈도수가 높은 value로 대체해준다.\n",
    "    dust_pivot_data_list = dust_pivot_data.columns.tolist()\n",
    "\n",
    "    for i in range(len(dust_pivot_data_list)):\n",
    "        if ('점검중' in dust_pivot_data.iloc[:,i].tolist()) == True:\n",
    "            dust_pivot_data.iloc[:,i] = \\\n",
    "            dust_pivot_data.iloc[:,i].apply(lambda x : pd.DataFrame(dust_pivot_data.iloc[:,i].value_counts()).index[0] if x == '점검중' else x)\n",
    "\n",
    "    #자치구 컬럼을 따로 떼어내고 그것을 데이터프레임을 만든다.\n",
    "    dust_jachigu = dust_pivot_data.iloc[:,0]\n",
    "    dust_series = pd.DataFrame(dust_jachigu)\n",
    "\n",
    "    #실시간 미세먼지 현시점 가장 최근 데이터 기준 49개 컬럼 맞춘 dataframe\n",
    "    dust_pivot_setting = dust_pivot_data.iloc[:,-49:]\n",
    "\n",
    "    # 자치구 1개와 시간데이터 49개를 합한 총 50개의 columns를 가진 데이터\n",
    "    dust_final = dust_series.join(dust_pivot_setting)\n",
    "\n",
    "    #미세먼지\n",
    "    #good은 0, bad는 1로 인코딩\n",
    "    for col in dust_final.columns.tolist()[1:]:\n",
    "        dust_final[col] = dust_final[col].apply(lambda x : 0 if x=='good' else 1)\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    #실시간 데이터에 실시간으로 존재하는 거치소들의 파생변수들을 merge로 추가해준다.\n",
    "    ddareng = pd.merge(df_total,now_final, on='대여소번호',how='left')\n",
    "\n",
    "    #체감온도와 날씨(맑음,흐림,비,눈)데이터 합치기.\n",
    "    weather_final = pd.merge(how_many_column, how_many_columns, on='권역명', how= 'left')\n",
    "\n",
    "    #미세먼지와 날씨(체감온도 + 날씨(맑음,흐림,비,눈))데이터 자치구 기준으로 합치기\n",
    "    air_weather = pd.merge(dust_final, weather_final, on = '권역명', how = 'left' )\n",
    "\n",
    "    air_weather.rename(columns = {'권역명':'자치구'},inplace=True)\n",
    "\n",
    "    #위에서 진행했던 ddareng(따릉이 실시간데이터 + 따릉이거치소관련 지역속성변수) 데이터와\n",
    "    #바로 위에 있는 air_weather(미세먼지실시간데이터 + 날씨실시간데이터(체감온도+기상상태) 데이터를 자치구 기준으로 합친다. \n",
    "    model_data = pd.merge(ddareng, air_weather, on='자치구',how='left')\n",
    "\n",
    "    if len(Deleted_node_set) != 0:\n",
    "        model_data = model_data[model_data.대여소번호.isin(list(Deleted_node_set)) == False]\n",
    "\n",
    "\n",
    "    #학습이 끝나고 최종적으로 여기에 예측값을 붙일 대여소명 리스트 따로선언.\n",
    "    bike_list = model_data.대여소이름.tolist()\n",
    "\n",
    "    #학습을 진행하는데 필요없는 컬럼삭제\n",
    "    model_data = model_data.drop([\"대여소번호\",\"위도\",\"경도\",\"geometry\",\"대여소이름\"],axis=1) \n",
    "\n",
    "    #따릉이 실시간 관련 컬럼들은 모델학습을 들어가게 되면 x_train, y_train으로 나눠야 해서 변동이 생기지만\n",
    "    #따릉이를 제외한 다른 모든 변수들은 삭제되거나 추가되는 변동사항이 없다. 따라서 따로 컬럼들을 리스트로 저장해둔다.\n",
    "\n",
    "    air_weather_columns = air_weather.columns.tolist()\n",
    "    geo_columns = now_final.columns.tolist()\n",
    "    not_dareng_columns = air_weather_columns + geo_columns\n",
    "    #학습에 필요없는 컬럼삭제\n",
    "    not_dareng_columns = set(not_dareng_columns) - set(['대여소번호','자치구','geometry', '대여소이름', '경도', '위도'])\n",
    "\n",
    "    #컬럼명에 ':' 가 들어가있으면 모델이 json형식으로 받아들여 에러가 나기때문에 다른 문자열로 대체해줘야한다. \n",
    "    #따라서 데이터프레임을 받고 컬럼명 내에 있는 ':' 를 '시'로 변경해서 데이터프레임을 뱉어주는 함수\n",
    "    def rename_col(df):\n",
    "        rename_columns_list = []\n",
    "        for i in df.columns.tolist():\n",
    "            try:\n",
    "                rename_columns_list.append(i.replace(':','시'))\n",
    "            except:\n",
    "                rename_columns_list.append(i)\n",
    "        df.columns = rename_columns_list\n",
    "        return df\n",
    "\n",
    "    #(현재 df_total의 0번째 컬럼은 대여소이름) \n",
    "    #df_total 인덱스 1부터 가장 최근인 컬럼을 제외한것 + 기타컬럼 => x_trian\n",
    "    x_train = model_data[df_total.columns.tolist()[1:-1] + list(not_dareng_columns)]\n",
    "    #컬럼명 수정\n",
    "    x_train = rename_col(x_train)\n",
    "    #df_total의 가장 최근 컬럼이 학습의 y_train값\n",
    "    y_train = model_data[df_total.columns.tolist()[-1]]\n",
    "    #컬럼명 수정\n",
    "    y_train = y_train.rename(y_train.name.replace(':','시'))\n",
    "    #미래를 예측할때 들어갈 데이터\n",
    "    x_test = model_data[df_total.columns.tolist()[2:] + list(not_dareng_columns)]\n",
    "    #컬럼명 수정\n",
    "    x_test = rename_col(x_test)\n",
    "\n",
    "    #xgboost는 모델에 넣을 데이터를 array형식으로 넣어줘야 하기에 데이터형식 변환.\n",
    "    x_train_n = np.array(x_train)\n",
    "    y_train_n = np.array(y_train)\n",
    "    x_test_n = np.array(x_test)\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from xgboost import XGBRegressor\n",
    "    import lightgbm as lgb\n",
    "    #ligthgbm 모델에 파라미터로 boosting_type='dart' 넣어주고 학습 평가지표는 rmse로 설정\n",
    "    lgbm = lgb.LGBMRegressor(random_state=2020,boosting_type='dart')\n",
    "    lgbm.fit(x_train, y_train,eval_metric='rmse')\n",
    "    simple_lgbm_pred = lgbm.predict(x_test)\n",
    "\n",
    "    #xgboost는 boosting_type 파라미터가 존재하지 않기때문에 없이 평가지표만 rmse로 설정.\n",
    "    xgb = XGBRegressor(random_state=2020)\n",
    "    xgb.fit(x_train_n, y_train_n,eval_metric='rmse')\n",
    "    simple_xgb_pred = xgb.predict(x_test_n)\n",
    "\n",
    "    #최종예측값\n",
    "    final_ensembled_prediction = (0.5*(simple_xgb_pred))+(0.5*(simple_lgbm_pred))\n",
    "    final_ensembled_prediction[final_ensembled_prediction < 0] = 0\n",
    "    final_ensembled_prediction = final_ensembled_prediction.round().astype(int)\n",
    "\n",
    "\n",
    "    import datetime as dt\n",
    "\n",
    "    dateval1 = datetime.strptime(df_total.columns.tolist()[-1].split('자')[0], \"%Y-%m-%d %H:%M\")\n",
    "    mm = dt.timedelta(minutes = 10)\n",
    "    predict_time = dateval1 + mm\n",
    "\n",
    "    final_df = pd.DataFrame({\"대여소이름\":bike_list, '예측잔여대수':final_ensembled_prediction.tolist()})\n",
    "    final_df['예측시간대'] = predict_time\n",
    "    final_df.대여소이름 = final_df.대여소이름.astype(str)\n",
    "    \n",
    "    \n",
    "    engine = create_engine(\"postgresql://postgres:6team123!@restored-aurora.cj92narf3bwn.ap-northeast-2.rds.amazonaws.com:5432/final_project\")\n",
    "\n",
    "    import psycopg2\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    final_df.to_sql(name = 'eval',\n",
    "              con = engine,\n",
    "              schema = 'public',\n",
    "              if_exists = 'append',\n",
    "              index = False)\n",
    "    \n",
    "    print('DB에 예측값 전송완료!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import atexit\n",
    "\n",
    "try:\n",
    "    sched = BackgroundScheduler(daemon=True)\n",
    "    sched.add_job(\n",
    "    predict_to_database, \n",
    "    trigger='cron',\n",
    "    minute='*/10',\n",
    "    hour='*'\n",
    "    )\n",
    "    sched.start()\n",
    "    atexit.register(lambda: sched.shutdown(wait=False))\n",
    "except:\n",
    "    print(\"Unexpected error:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import schedule\n",
    "schedule.every(10).minutes.do(predict_to_database)\n",
    " \n",
    "#실제 실행하게 하는 코드\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
